{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">IS590 Programmming for Analytics - Summer - Final Project </p>\n",
    "\n",
    "### <p style=\"text-align: center;\">Land Use Analysis Leveraging Numpy & Point Cloud Generated Scans</p>\n",
    "**<p style=\"text-align: center;\">Author - Jeremy Carnahan</p>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Land Use Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aerial imagery has been used for many years, [predating even the airplane](https://en.wikipedia.org/wiki/Pigeon_photography), as people have used aerial perspectives for many industries.  One usecase in particular is for quickly accounting for how land is partitioned into uses such as farming, forestry, urbanization, roadways, etc. Flood planning utilize this distribution of land use to determine rainfall absorption and runoff for hydrology models.\n",
    "\n",
    "For almost a century, measurement of land use has utilized a process called photogrammetry to extract surface areas of land use categories by measuring aerial photos using trigonometry.  This process is still quite effective, but is dependent on knowing the precise altitude above ground and camera intrinsics.  In the past few decades, a new technology called LiDAR (Light Detection And Ranging) leverages numerous laser pulses to scan terrain from the air, and return very accurate representation of features on the ground.  The data this produces is called a **pointcloud**, which is a 3-dimensional array of points in a Cartesian grid.  Further, this pointcloud undergoes a classification process using a standard called [American Society for Photogrammetry & Remote Sensing v1.4](http://www.asprs.org/wp-content/uploads/2019/03/LAS_1_4_r14.pdf).  This standard is used to delineate land use categories below, either through manual categorization or machine learning, to about 20 categories.  In the flood planning use case, typically the three categories used for hydrology modeling are: vegetation, road surfaces, and ground.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "_As pointclouds are classified 3-dimensional XYZ points using ASCII format, and are generally several gigabytes to terrabytes in size.  My hypothesis is that we can use Numpy to analyze these points to more quickly determine land use surface area, as Numpy is designed for n-dimension vectorization, and efficiently leverages memory for quick analysis.  If this approach is successful, it could substitute the numerous hours of manual measurement of aerial photography to determine vegegation and road surface area for flood modeling._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code\n",
    "###### Import Modules\n",
    "Here we need to pull in the modules we'll use to extract the pointcloud (.las file) into a proper Numpy ndArray using 'laspy'. \n",
    "From 'scipy' we'll use functions to calculate the area of classified points. \n",
    "'ipyleaflet' is used to get a course understanding of the area of interest by embedding a map we can calculate total area from.\n",
    "'skimage.measure' is a module we can use to decimate our pointcloud for better visualization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import laspy\n",
    "from laspy import header\n",
    "import scipy\n",
    "import numpy as np\n",
    "# ipyleaflet can be found by using 'conda install -c conda-forge ipyleaflet' in Anaconda command prompt\n",
    "import ipyleaflet\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import path\n",
    "from skimage.measure import block_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Area of Interest and Map Extent\n",
    "In order to download the LiDAR data from the [National Oceanic & Atmospheric Administration](https://coast.noaa.gov/dataviewer/#/lidar/search/), we need to determine the extent (bounding box) of the geography of interest.  ipyleaflet, which uses an open source tile mapping service using a product called Leaflet, can be embedded in Jupyter with the capability to determine overall surface area for the area of interest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e265b4f512c34511962b736a5cffc134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[29.608362, -95.158648], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Leaflet opensource mapping tile service.  Used ipyleaflet docs here:\n",
    "# https://ipyleaflet.readthedocs.io/en/latest/api_reference/map.html#usage \n",
    "# Added a polygon around the area of interest with ipyleaflet docs here:\n",
    "# https://ipyleaflet.readthedocs.io/en/latest/api_reference/rectangle.html?highlight=draw%20polygon \n",
    "# Added an in-frame measurement tool from ipyleaflet docs here:\n",
    "# https://ipyleaflet.readthedocs.io/en/latest/api_reference/measure_control.html?highlight=calculate%20area#methods\n",
    "m = ipyleaflet.Map(center=(29.608362, -95.158648), zoom=13)\n",
    "\n",
    "\n",
    "rectangle = ipyleaflet.Rectangle(bounds=((29.59172203, -95.17940998), (29.62288498, -95.13817692)))\n",
    "m.add_layer(rectangle)\n",
    "\n",
    "measure = ipyleaflet.MeasureControl(\n",
    "    position='bottomleft',\n",
    "    active_color = 'orange',\n",
    "    primary_length_unit = 'kilometers',\n",
    "    primary_area_unit = 'sqmeters'\n",
    ")\n",
    "m.add_control(measure)\n",
    "\n",
    "measure.completed_color = 'blue'\n",
    "\n",
    "measure.add_area_unit('sqmeters', 1, 4)\n",
    "measure.secondary_area_unit = 'sqmiles'\n",
    "\n",
    "m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing the Pointcloud .las file\n",
    "Now with our area of interest defined, and LiDAR data downloaded from NOAA, we can import the pointcloud using laspy to convert the .las ASCII file into a Numpy 3D array. \n",
    "\n",
    "Note: We'll load a significantly trimmed down .las pointcloud (~13Mb) from 3GB to allow for storage in Github.  However, as noted above, Numpy was chose because of its efficiency in computing large files.\n",
    "\n",
    "We'll also do a little data exploration to determine the ASPRS standard version used, header, and point metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laspy_infile(directory: str) -> laspy.file.File:\n",
    "    infile = laspy.file.File(directory, mode='r')\n",
    "    return infile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the .las file to laspy file format and check details\n",
    "# Based on laspy.py 'getting started' docs found here:\n",
    "# https://pythonhosted.org/laspy/tut_part_1.html\n",
    "\n",
    "def get_las_meta(inFile: laspy.file.File) -> list:\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    # We need to know what version of .las we're using so that we associate the right classification numbers\n",
    "    # https://pythonhosted.org/laspy/_modules/laspy/header.html\n",
    "    major = header.HeaderManager.get_majorversion(inFile)\n",
    "    minor = header.HeaderManager.get_minorversion(inFile)\n",
    "    las_version = str(major) + '.' + str(minor)\n",
    "    metadata.append(las_version)\n",
    "\n",
    "    # Here we look at the header and point formats to know what features are available\n",
    "    # Code examples used from laspy.py doc page here:\n",
    "    # https://pythonhosted.org/laspy/tut_part_1.html \n",
    "\n",
    "    # Header Fields\n",
    "    header_data = []\n",
    "    headerformat = inFile.header.header_format\n",
    "    for spec in headerformat:\n",
    "        header_data.append(spec.name)\n",
    "\n",
    "    # Metadata explaination from Laspy documentation here:\n",
    "    # https://pythonhosted.org//laspy/tut_background.html?highlight=classification\n",
    "    # Point Metadata\n",
    "    point_data = []\n",
    "    pointformat = inFile.point_format\n",
    "    for spec in inFile.point_format:\n",
    "        point_data.append(spec.name)\n",
    "    \n",
    "    metadata.append(header_data)\n",
    "    metadata.append(point_data)\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get our LAS to Laspy file object back and set as a global variable so we can look at its metadata in more detail, and call it for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "laspy_infile = get_laspy_infile('./Data/ellington-segment.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAS Version: 1.4\n",
      "\n",
      "Header Data: ['file_sig', 'file_source_id', 'global_encoding', 'proj_id_1', 'proj_id_2', 'proj_id_3', 'proj_id_4', 'version_major', 'version_minor', 'system_id', 'software_id', 'created_day', 'created_year', 'header_size', 'data_offset', 'num_variable_len_recs', 'data_format_id', 'data_record_length', 'legacy_point_records_count', 'legacy_point_return_count', 'x_scale', 'y_scale', 'z_scale', 'x_offset', 'y_offset', 'z_offset', 'x_max', 'x_min', 'y_max', 'y_min', 'z_max', 'z_min', 'start_wavefm_data_rec', 'start_first_evlr', 'num_evlrs', 'point_records_count', 'point_return_count']\n",
      "\n",
      "Point Metadata: ['X', 'Y', 'Z', 'intensity', 'flag_byte', 'classification_flags', 'classification_byte', 'user_data', 'scan_angle', 'pt_src_id', 'gps_time', 'red', 'green', 'blue']\n"
     ]
    }
   ],
   "source": [
    "las_metadata = get_las_meta(laspy_infile)\n",
    "print('LAS Version: {}'.format(las_metadata[0]))\n",
    "print()\n",
    "print('Header Data: {}'.format(las_metadata[1]))\n",
    "print()\n",
    "print('Point Metadata: {}'.format(las_metadata[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Size & Shape\n",
    "Now that we've seen the metadata, we'll start to transform the raw Numpy extract from the .las in a more usable format.  Here we'll also take a look at the data's shape and size.\n",
    "\n",
    "laspy.py documentation recommends extracting out each dimension from the .las file first, stacking, and transposing.  This is due to the way that the LiDAR systems organze the XYZ points in an orientation that differs from Numpy's XYZ/3D array. \n",
    "\n",
    "Recommendation found on laspy.py getting started page here: https://pythonhosted.org/laspy/tut_part_1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transposed_data type is: <class 'numpy.ndarray'>\n",
      "transposed_data shape is: (378653, 3)\n",
      "transposed_data size is: 1135959\n",
      "\n",
      "\n",
      "Quick look with a head of 10 of XYZ points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.99823420e+05, 3.19961806e+06, 8.64000034e+00],\n",
       "       [1.99823951e+05, 3.19961821e+06, 8.67000007e+00],\n",
       "       [1.99823631e+05, 3.19961857e+06, 8.64000034e+00],\n",
       "       [1.99823342e+05, 3.19961892e+06, 8.64000034e+00],\n",
       "       [1.99824420e+05, 3.19961828e+06, 8.64000034e+00],\n",
       "       [1.99824131e+05, 3.19961860e+06, 8.64000034e+00],\n",
       "       [1.99823849e+05, 3.19961896e+06, 8.64000034e+00],\n",
       "       [1.99823552e+05, 3.19961932e+06, 8.64000034e+00],\n",
       "       [1.99824990e+05, 3.19961832e+06, 8.64000034e+00],\n",
       "       [1.99824709e+05, 3.19961867e+06, 8.64000034e+00]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transpose_las(inFile: laspy.file.File) -> np.array:\n",
    "    # If you are interested in seeing how the data looks when not transposed, you can uncomment the block below.  Note that this \n",
    "    # will double the memory used for this project around 3GB.\n",
    "    # original_data = np.array([inFile.x, inFile.y, inFile.z])\n",
    "    # print(original_data.shape)\n",
    "    # print('original_data type is: ' + str(type(original_data)))\n",
    "\n",
    "    transposed_las = np.vstack([inFile.x, inFile.y, inFile.z]).transpose()\n",
    "    return transposed_las\n",
    "\n",
    "transposed_data = transpose_las(laspy_infile)\n",
    "transposed_data_shape = transposed_data.shape\n",
    "transposed_data_size = transposed_data.size\n",
    "print('transposed_data type is: ' + str(type(transposed_data)))\n",
    "print('transposed_data shape is: ' + str(transposed_data_shape))\n",
    "print('transposed_data size is: ' + str(transposed_data_size))\n",
    "print()\n",
    "print()\n",
    "print('Quick look with a head of 10 of XYZ points')\n",
    "transposed_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Transformation\n",
    "As noted in the background section above, we are only interested in vegetation and road surfaces when determining land use for precipitation absorption and runoff (note that other hydrology methods to determine water flow can be derived from a pointcloud, but is beyond the scope of this analysis).  \n",
    "\n",
    "Our first step is to extract just the classification we are interested in, and we'll save them out to files and load the points back in, which helps us manage our memory more effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(inFile: laspy.file.File, features: list):\n",
    "    # Used a variation of this code to identify and aggregate LiDAR classifications\n",
    "    # https://gis.stackexchange.com/questions/255833/classifying-lidar-ground-points-using-laspy\n",
    "    # Categories takent from ASPRS Standard 1.4 http://www.asprs.org/wp-content/uploads/2019/03/LAS_1_4_r14.pdf    \n",
    "    \n",
    "    for feature in features:\n",
    "        if feature == 'ground':                 \n",
    "            land_use = np.where(inFile.Classification == 2)  # ground (2)\n",
    "        elif feature == 'vegetation':\n",
    "            # All vegetation is low (3), medium (4), and high classified vegetation (5) \n",
    "            land_use = np.where(np.logical_or(inFile.Classification == 3, inFile.Classification == 4, inFile.Classification == 5))\n",
    "        elif feature == 'buildings':            \n",
    "            land_use = np.where(inFile.Classification == 6)  # buildings (6)\n",
    "        elif feature == 'water':   \n",
    "            water = np.where(inFile.Classification == 9)  # water (9)\n",
    "        elif feature == 'rail':\n",
    "            rail = np.where(inFile.Classification == 10)  # rail (10)\n",
    "        elif feature == 'roads':\n",
    "            land_use = np.where(inFile.Classification == 11)   # road surfaces (11)\n",
    "        elif feature == 'wires':\n",
    "            land_use = np.where(np.logical_or(inFile.Classification == 13, inFile.Classification == 14)) # wires (13, 14)\n",
    "        elif feature == 'transmission_tower':\n",
    "            transmission_tower = np.where(inFile.Classification == 15)  # transmission tower (15)\n",
    "        elif feature == 'bridge':\n",
    "            bridge = roads = np.where(inFile.Classification == 17)  # bridge (17)\n",
    "        elif feature == 'overhead':\n",
    "            # overhead equipment (19) (e.g. conveyors, mining equip., traffic lights)\n",
    "            overhead = np.where(inFile.Classification == 19)\n",
    "        elif feature == 'snow':\n",
    "            snow = np.where(inFile.Classification == 21)  # snow (21)\n",
    "\n",
    "            # Outfile write to .las found on Laspy github example here:\n",
    "            # https://github.com/laspy/laspy\n",
    "        outFile = laspy.file.File('./Data/'+ feature + '.las', mode='w', header=inFile.header)\n",
    "        outFile.points = inFile.points[land_use]\n",
    "        outFile.close()\n",
    "    \n",
    "    return True\n",
    "\n",
    "extract_features(laspy_infile, ['ground', 'vegetation', 'roads'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Analyze New Arrays\n",
    "Just a quick check that we've extracted the classifications that we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read back in just the elements we want to calculate the surface area for\n",
    "inFile_veg= laspy.file.File('./vegetation.las', mode='r')\n",
    "vegetation_np = np.vstack([inFile_veg.x, inFile_veg.y, inFile_veg.z]).transpose()\n",
    "veg_shape = vegetation_np.shape\n",
    "print('vegetation_np shape is: ' + str(veg_shape))\n",
    "veg_percent = veg_shape[0]/transposed_data_shape[0]\n",
    "print('percentage of total points = ' + '{:.2%}'.format(veg_percent))\n",
    "print()\n",
    "\n",
    "inFile_roads= laspy.file.File('./roads.las', mode='r')\n",
    "roads_np = np.vstack([inFile_roads.x, inFile_roads.y, inFile_roads.z]).transpose()\n",
    "roads_shape = roads_np.shape\n",
    "print('roads_np shape is: ' + str(roads_shape))\n",
    "roads_percent = roads_shape[0]/transposed_data_shape[0]\n",
    "print('percentage of total points = ' + '{:.2%}'.format(roads_percent))\n",
    "print()\n",
    "\n",
    "inFile_ground= laspy.file.File('./ground.las', mode='r')\n",
    "ground_np = np.vstack([inFile_ground.x, inFile_ground.y, inFile_ground.z]).transpose()\n",
    "ground_shape = ground_np.shape\n",
    "print('ground_np shape is: ' + str(ground_shape))\n",
    "ground_percent = ground_shape[0]/transposed_data_shape[0]\n",
    "print('percentage of total points = ' + '{:.2%}'.format(ground_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Visualization\n",
    "Here we start to run into some issues.  While so far the analysis of the pointclouds in numpy ndarrays has been smooth, with ample memory, as soon as we begin to try to visualize the data in Jupyter the system begins to stagger.  Additionally, the limited screen space limits any detailed view we may want to have to examine the pointcloud.\n",
    "\n",
    "Below are some attempts to visualize the data, but ultimately I chose to use an external tool called 'CloudCompare' to view the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = roads_np[:,0]\n",
    "x = roads_np[:,1]\n",
    "z = roads_np[:,2]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=[500, 300])\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, marker='.')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pointcloud Decimation (Reduction)\n",
    "Numpy arrays make it fairly easy to perform a reduction in the granularity of the pointcloud.  While we wouldn't want to do this if we want to maintain accuracy/precision, it may be a useful step for reducing the size for visualization.  Below I've used a skimage.measure method called 'block_reduce' that systematically reduces the size (and shape if desired) using an average of points every 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used a function from scikit-image docs to decimate the pointcloud for better visualization\n",
    "# https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.block_reduce\n",
    "\n",
    "reduce = block_reduce(roads_np, block_size=(100, 1), func=np.mean, cval=np.mean(roads_np))\n",
    "print(reduce.shape)\n",
    "print(type(reduce))\n",
    "\n",
    "y = roads_np[:,0]\n",
    "x = roads_np[:,1]\n",
    "z = roads_np[:,2]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=[500, 300])\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, marker='.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Land Use Surface Area (and some bad news...)\n",
    "Now that we've extracted out just the land use points/elements that we want, we now need to perform our area calculations.  'scipy.spatial' has a method called ConvexHull, which we've seen previously in IS-590; however, this is a little different.  Even though the method is called ConvexHull, when given a 3D numpy array, the method actually peforms what is called a Delaunay triangulation invoked from Qhull class parent.  Effectively it connects the points with edges to create small triangle facets, and then calculates the area of each small triangle.  This has the added benefit over a simple aerial overlay in that Delaunay will also include additional area as it considers the z-axis in addition to just the x & y axis.  But there is bad news...\n",
    "\n",
    "Unfortunately, it seems the scipy.spatial.ConvexHull method is unable to determine when it should not connect points that are not near to it.  A function that is used in pointcloud utilities (but not present in scipy) is to restrict the Delaunay to a preset threshold of \"nearest neighbors\", which results in **MANY** large facet areas to be calculated and inflating our area results.  As we see in the ConvexHull results below, the area calculated is many times larger than our starting surface area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We use a ConvexHull method from scipy.spatial which is actually invoking a Delaunay triangulation process to create facets\n",
    "# and calculate total area.\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.html \n",
    "veg_delaunay = scipy.spatial.ConvexHull(vegetation_np)\n",
    "roads_delaunay = scipy.spatial.ConvexHull(roads_np)\n",
    "ground_delaunay = scipy.spatial.ConvexHull(ground_np)\n",
    "\n",
    "\n",
    "print('Vegetation Area = ' + '{:,}'.format(veg_delaunay.area))\n",
    "print('Road Surface Area = ' + '{:,}'.format(roads_delaunay.area))\n",
    "print('Ground Area = ' + '{:,}'.format(ground_delaunay.area))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "[Pigeon Photography](https://en.wikipedia.org/wiki/Pigeon_photography)\n",
    "\n",
    "[NOAA Data Access Viewer](https://coast.noaa.gov/dataviewer/#/lidar/search/)\n",
    "\n",
    "[ASPRS LAS Specification 1.4 - R14](http://www.asprs.org/wp-content/uploads/2019/03/LAS_1_4_r14.pdf)\n",
    "\n",
    "[Leaflet Opensource Tile Mapping for Python](https://ipyleaflet.readthedocs.io/en/latest/api_reference/map.html#usage)\n",
    "\n",
    "[Laspy .las Editor for Python](https://pythonhosted.org/laspy/tut_part_1.html)\n",
    "\n",
    "[Pointcloud Classification Parsing](https://gis.stackexchange.com/questions/255833/classifying-lidar-ground-points-using-laspy)\n",
    "\n",
    "[block_reduce from scikit-learn](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.block_reduce)\n",
    "\n",
    "[ConbexHull/Delaunay from scipy.org docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.html)\n",
    "\n",
    "[Github Markdown Cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
